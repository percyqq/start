红黑树，算法，大流量任务处理，b+树特点，jdk排序，归并排序


绩效提成：
    订单服务。DTS数据流转至 mongo
    拉取数据分布式计算。

网关：api鉴权。登录验证，请求转发，限流
    技术选型
    网关除了满足功能上的需求外, 性能上的需求也需要着重考虑, 毕竟作为各个业务系统对外的唯一入口, 
        网关的性能可能会成为整个业务系统的瓶颈. 业务并不复杂, 性能要求高, 响应式编程正是一个不错的选择.
    1. Spring WebFlux + netty: 响应式Web框架.
    2. Spring Data Reactive Redis + Lettuce: 响应式redis客户端.
    3. Guava: Google工具包, 使用LoadingCache作为进程内缓存.
    
    
开放API网关实践(三) —— 限流
https://blog.csdn.net/piaoruiqing/article/details/101798512
1. 令牌桶     Guava提供了令牌桶算法的实现.

2.分布式限流
    想要在集群中进行全局限流, 其关键在于将限流信息记录在共享介质中, 如Redis、memcached等. 
        为了将限流做的精确, 写必须是原子操作.
[Redis+Lua是一个不错的选择, 示例Lua脚本如下:]
        local key = KEYS[1] -- 限流的KEY
        local limit = tonumber(ARGV[1]) -- 限流大小
        local current = tonumber(redis.call('get', key) or '0')
        if current + 1 > limit then
            return 0
        else
            redis.call('INCRBY', key,'1')
            redis.call('expire', key,ARGV[2])   -- 过期时间
            return current + 1
        end
    1.分布式限流将令牌的发放放到共享介质中.
    2.获取(消费)令牌操作必须是原子的.
    3.共享介质要高可用(Redis集群)


高可用网关
 Eureka   和 zookeeper

Eureka AP
 Eureka Server 也可以运行多个实例来构建集群，解决单点问题，但不同于 ZooKeeper 的选举 leader 的过程，
    Eureka Server 采用的是Peer to Peer 对等通信。这是一种去中心化的架构，无 master/slave 之分，
    每一个 Peer 都是对等的。在这种架构风格中，节点通过彼此互相注册来提高可用性，
    每个节点需要添加一个或多个有效的 serviceUrl 指向其他节点。每个节点都可被视为其他节点的副本。
Eureka的集群中，只要有一台Eureka还在，就能保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证强一致性）。
    除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，
    那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况：
        1.Eureka不再从注册表中移除因为长时间没有收到心跳而过期的服务；
        2.Eureka仍然能够接受新服务注册和查询请求，但是不会被同步到其它节点上（即保证当前节点依然可用）；
        3.当网络稳定时，当前实例新注册的信息会被同步到其它节点中；

Zookeeper CP
  与 Eureka 有所不同，Apache Zookeeper 在设计时就紧遵CP原则，即任何时候对 Zookeeper 的访问请求能得到一致的数据结果，
    同时系统对网络分割具备容错性，但是 Zookeeper 不能保证每次服务请求都是可达的。
    从 Zookeeper 的实际应用情况来看，在使用 Zookeeper 获取服务列表时，如果此时的 Zookeeper 集群中的 Leader 宕机了，
    该集群就要进行 Leader 的选举，
    又或者 Zookeeper 集群中半数以上服务器节点不可用（例如有三个节点，如果节点一检测到节点三挂了 ，节点二也检测到节点三挂了，
        那这个节点才算是真的挂了），那么将无法处理该请求。所以说，Zookeeper 不能保证服务可用性。

Consul 
  内置了服务注册与发现框架、分布一致性协议实现、健康检查、Key/Value 存储、多数据中心方案，
        不再需要依赖其他工具（比如 ZooKeeper 等），使用起来也较为简单。
  Consul 遵循CAP原理中的CP原则，保证了强一致性和分区容错性，且使用的是Raft算法，比zookeeper使用的Paxos算法更加简单。
    虽然保证了强一致性，但是可用性就相应下降了，例如服务注册的时间会稍长一些，
    因为 Consul 的 raft 协议要求必须过半数的节点都写入成功才认为注册成功 ；
    在leader挂掉了之后，重新选举出leader之前会导致Consul 服务不可用。


#高并发优化吗？能抗下每秒上万并发吗？
大公司一般有分布式kv存储，tair，redis，mongodb，高并发，每秒几万几十万都没问题，甚至每秒百万
    实时库存数据放kv存储里去，先查库存再扣减库存，你在操作库存的时候，直接扣减，如果你发现扣减之后是负数的话，
        此时就认为库存超卖了，回滚刚才的扣减，返回提示给用户。
    对kv做的库存修改写MQ，异步同步落数据库，相当于异步双写，用分布式kv抗高并发，做好一致性方案
    
#分段加锁
    库存1W。分成10个表来扣减。1个扣减不够，尝试多个库存锁合并扣减


https://github.com/shishan100/Java-Interview-Advanced  
	Java进阶面试训练营  

https://mp.weixin.qq.com/s?__biz=MzU0OTk3ODQ3Ng==&mid=2247485090&idx=1&sn=5502672d9bf52551038b911d7ab623b9&chksm=fba6eea1ccd167b73a542b76dad423da21a2b452f768aac996cb50eeac1adc5d4978afe762ce&mpshare=1&scene=1&srcid=0608mo3eKwh686hXNzvRQfEB%23rd


SpringMVC
    1.DispatcherServelet  --> 
    2. HandlerMapping ==> 
        处理映射器，根据请求的url映射到对应的处理器(Controller)的某个处理方法上，
        映射成功后返回一个HandlerExecutionChain对象（处理器执行链），其中包括处理器（Controller）以及拦截器。
    3. HandlerAdapter：处理器适配器，使用了适配器模式，将处理器映射器找到的处理器对象和处理方法，
        适配成DispatchServlet所需要的请求处理方法（目标接口），
        这样DispatchServlet调用统一的请求处理接口完成请求处理，
        处理器适配器HandlerAdapter的功能就是将我们自定义的前端控制器Controller适配成DispatchServlet需要的接口。   
    4、ViewResolver：视图解析器，请求处理完后，DispatchServlet会受到请求处理结果（包括模型数据和逻辑视图），
        DispatchServlet会找到Spring容器中的ViewResolver对象将逻辑视图解析成物理页面文件。
        注意对于使用了@ResponseBody注解的方法，DispatchServlet会使用合适的消息转换器将请求处理结果转换后返回给前台。　
    5、DispatchServlet找到物理视图文件后，DispatchServlet使用视图模板引擎将模型数据渲染到物理视图页面中，
        最后DispatchServlet将渲染后的页面返回给前台，请求结束。　 
        

       
策略模式 、 工厂模式的区别
    工厂模式是创建型模式 ，它关注对象创建，提供创建对象的接口，让对象的创建与具体的使用客户无关。 
    策略模式是对象行为型模式 ，它关注行为和算法的封装 。
        再举个例子，还是我们出去旅游，对于策略模式我们只需要选择其中一种出行方法就好，
        但是工厂模式不同，工厂模式是你决定哪种旅行方案后，由工厂代替你去构建具体方案（工厂代替你去买火车票）。


不同版本的并发hashmap区别
jdk1.7	内部采用了锁分段机制来替代hashtable的独占锁，从而提高了性能 segment[]数组和HashEntry[]数组
jdk1.8	进行put操作时：上面的segment采用的是cas机制来保证线程安全的。底层还是用 数组+链表–>红黑树 实现（下面这块还是使用synchronized锁）


2. 事务的ACID，其中把事务的隔离性详细解释一遍
3. 脏读、幻影读、不可重复读
4. 红黑树、二叉树的算法
5. 平常用到哪些集合类？ArrayList和LinkedList区别？HashMap内部数据结构？ConcurrentHashMap分段锁？
6. jdk1.8中，对hashMap和concurrentHashMap做了哪些优化
7. 如何解决hash冲突的，以及如果冲突了，怎么在hash表中找到目标值
8. synchronized 和 ReentranLock的区别？yang
9. ThreadLocal？应用场景？
10. Java GC机制？GC Roots有哪些？
    GC Roots 包括（但不限于）如下几种：
    Java 方法栈桢中的局部变量；已加载类的静态变量；JNI handles  native方法中的；已启动且未停止的 Java 线程。
    可达性分析可以解决引用计数法所不能解决的循环引用问题。
    举例来说，即便对象 a 和 b 相互引用，只要从 GC Roots 出发无法到达 a 或者 b，那么a和b就是死亡的对象。
    
    在 Java 虚拟机里，传统的垃圾回收算法采用的是一种简单粗暴的方式，那便是 Stop-the-world，停止其他非垃圾回收线程的工作，
        直到完成垃圾回收。这也就造成了垃圾回收所谓的暂停时间（GC pause）。
    Java 虚拟机中的 Stop-the-world 是通过安全点（safepoint）机制来实现的。
    当 Java 虚拟机收到 Stop-the-world 请求，它便会等待所有的线程都到达安全点，才会停止所有线程，
        并允许请求Stop-the-world的那个线程进行独占的工作。

Java 虚拟机会记录 Survivor 区中的对象一共被来回复制了几次。如果一个对象被复制的次数为 15，
可以通过虚拟机参数 -XX:+MaxTenuringThreshold进行设置。



11. MySQL行锁是否会有死锁的情况？

二面
1. 乐观锁和悲观锁了解吗？JDK中涉及到乐观锁和悲观锁的内容？
2. Nginx负载均衡策略？
3. Nginx和其他负载均衡框架对比过吗？
4. Redis是单线程？
5. Redis高并发快的原因？
6. 如何利用Redis处理热点数据
7. 谈谈Redis哨兵、复制、集群
8. 工作中技术优化过哪些？JVM、MySQL、代码等都谈谈

三面
1. Spring Cloud用到什么东西？如何实现负载均衡？服务挂了注册中心怎么判断？
2. 网络编程nio和netty相关，netty的线程模型，零拷贝实现
3. 分布式锁的实现你知道的有哪些？具体详细谈一种实现方式
4. 高并发的应用场景，技术需要涉及到哪些？怎样来架构设计？
5. 接着高并发的问题，谈到了秒杀等的技术应用：kafka、redis、mycat等
6. 最后谈谈你参与过的项目，技术含量比较高的，相关的架构设计以及你负责哪些核心编码

最新美团面试题目，技术主要是3面，重点问了：

1.Java容器的问题：hashmap、currenthashmap等，建议面试前把这几个问到最多的类的原理、到底层数据结构、再到数据扩容，以及算法复杂度，都需要重点掌握。
2.Java相关的：线程锁、以及线程流转图、线程池等。
3.JVM不用说了，每次都考，如果对内存的回收，垃圾回收器的种类区别，回收算法机制这个必须要掌握。
4.数据库MySQL相关的，这个也是每次必问，毕竟是平时工作中使用最多的，考察数据库基本功：存储引擎、SQL查询优化、常见索引的使用和区别、事务的使用，表范式设计，以及分库分表的策略和实际应用等。
5.分布式也是必考的系列，问得最多就是Redis、Dubbo等，面试官主要就是考察缓存、RPC的实际使用情况。
6.如果面试前，还能对常见的高并发的场景，以及技术方案熟悉，那面试会好很多。



# start
1st start


内存模型，类加载机制，JVM性能优化，多线程
分布式缓存，消息，负载均衡
sql

RabbitMQ：		并非 分布式，镜像集群模式，每个节点都有全部数据。如果消息数据量非常大，没法玩，所以不是分布式~
	拆分成多个queue，按业务拆分消息，比如当前业务的操作（比如一个订单，1.扣库存，2.生成订单，3.扣钱，4.日志。  放到一个queue里面去，）
	1个业务操作1个queue

Kafka：  纯分布式	。 单线程消费，处理1条消息 几十ms，1秒钟大概处理10+条数据。多线程最高（4核8G，单机，32线程）可以做到上千~ 
	可以保证写入一个partition中的数据一定有顺序
	生成数据在写的时候，指定一个key，这个业务的所有消息都选择当前key
	
	一个partition只能被一个消费者去消费。消费者取出来的时候一定是有序的。
	但是消费者内部一般都是多线程去消费消息。提高吞吐
	再设计一个内存队列，比如按订单id，作为key，那么同一个业务的所有消息都咋一个内存队列中。O了
高可用： 每台机器启动一个broker，多个broker组成，每个broker是集群中的一个节点；创建以后topic，这个topic可以划分多个partition，
		 每个partition可以存在不同的broker上，每个partition就放一部分数据
	HA，replica副本机制。选举一个leader出来。
	
	
	
Redis 单线程为何还支持高并发。  单机redis 的QPS: 上万

1.纯内存操作
2.非阻塞的IO操作，由IO多路复用程序，交给队列转发给 事件处理器 来处理
3.单线程避免了多线程的频繁上下文切换的问题

	NIO 异步
  reactor模式
file event handler 文件事件处理器  --> 单线程，IO多路复用，同时监听多个socket。


一、复制的流程
1.slave node启动，仅保存master node（host,ip）的信息，conf文件中读取到。
2.slave node内部有定数任务，检查是否有master要连接，如果有，则建立socket网络连接。
3.与master口令认证~如果有的话
4.master第一次全量复制，将自己的的所有数据发送给slave。
5.master接受到新数据，异步发送新写的数据到 slave。

二、数据同步的核心机制
	slave第一次连接master的时候的细节。
	master自身不断累加offset，slave也会在自身不断累加offset，slave会不断上报自己的offset到master。
	master和slave知道各种数据的offset，才知道互相之间数据不一致的情况

三、backlog
master log有一个，默认的backlog，默认是1MB
master node在给slave node 复制数据时，也会将数据在backlog 中同步写一份。
backlog主要是用来做全量复制中的 增量复制的。

master run id	类似于pid			使用rdb恢复，run id会变化。
输入命令 info server， 主要是针对master（ip + port + run id）重启的情况，run id不一致会触发全量复制

四、psync			psync runid offset
slave使用psync从master进行复制，master根据自身的情况返回响应信息，可能是fullResync，  也可能是continue增量复制

全量复制和增量复制
master bgsave --> rdb,  repl-timeout， rdb的复制文件时间，默认60s，比如千M网卡，6G的rdb文件就容易超时，如要设置大值。
增量的数据缓存在内存中， client-output-buffer-limit，256MB 64MB 0  :  内存缓存区超过某个值64，或者一次性超过某个值256，或者超过

4-6G大概需要花费1-2min。

quorum 哨兵数量，
min-slaves-to-write  	1		至少有1个slave，数据的同步和复制不能超过10s、
min-slaves-max-lg 		10  	10s内没有收到slave的数据，认为末端不可用，停止master的写。


一致性hash，一个环上n个节点。 当一台宕机，失效流量1/n。
	问题：可能集中在某个hash区间类的值特别多，导致失效后大量数据涌入，造成性能瓶颈。

	虚拟节点： 每个master做了均匀分布的虚拟节点。   原来的环是  :   
	   2	
     /   \
	/     \
   1 ————  3			
	
	    2	
       / \
	 [1]  \   
	 /    [1]
	/       \
   1 ——[1]—— 3

Redis Cluster有固定的16384个hash slot。 对每个key计算CRC16值，然后对16384取模，可以获取key对应的hash slot。
每个master都持有部分slot，比如3个master，那每个就持有 16384/3 个hash slot。
增加一个master，将其他master的hash slot移动部分过去，减少一个master，讲它的移动到其他master。
移动hash slot 的成本非常低。
客户端的api，可以对应指定的数据，让他们走同一个hash slot，通过hash tag 实现。


1、使用Zookeeper。集中式，更新和读取的时效性很好。所有的元数据更新压力集中在一个地方
2、节点之间是有goossip协议通信。【小道流言~】  元数据更新比较分散，更新请求陆陆续续，有延时，降低压力。	
所有节点都持有一份元数据，当前节点变更，发给其他所有节点
	meet， 添加节点
	ping，心跳，信息，元数据交换 
	pong，广播和更新
	fail



缓存雪崩，穿透~
	事前高可用，cluster + 主从。 ehcache可以做个缓存
	事中 hystrix限流+ 降级
	事后，redis需要持久化。尽快恢复缓存集群。


apt-get install 	yum,vim,ssh,net-tools,iputils-ping

docker commit -a "QQ" -m "add utils" imageID redis:v1

Dokcerfile
	FROM redis:v1
	MAINTAINER QQ
	ADD run.sh /run.sh
	RUN chmod 755 /run.sh
	EXPOSE 22
	ENTRYPOINT ["/run.sh"]

docker build -t ubuntu/ssh:v1.1 .

docker save cid -o xxx.tar
docker load -i  xxx.tar

passwd 修改密码

docker run -it --network host --name UUU d46d65671340 /bin/bash

Docker容器里的centos、unbuntu无法使用 systemctl 命令的解决方案		解决方案：/sbin/init	--privilaged=true一定要加上的
System has not been booted with systemd as init system (PID 1). Can't operat
	docker run -it --privileged=true --network bridge --name UUU -p 127.0.0.1:10222:22 9a73f67605c5 /sbin/init
	
docker run -it -p 127.0.0.1:10122:22 d46d65671340 /bin/bash

docker ps  
docker exec -it 775c7c9ee1e1 /bin/bash 
docker run -it --name ubuntu_dev -p 127.0.0.1:10022:22 -p 6379:6379 ubuntu/dev:v1.2 /bin/bash
docker run -it --name ubuntu_dev  ubuntu/dev:v1.2 /bin/bash

export http_proxy=http://Q00xxxxx:password\!@proxy.xx.com:8080/

docker commit -a "QQ" -m "add utils" c25bd2b201dd redis:v1.1


docker network create --subnet=172.10.0.0/16 redis-cluster-network
docker run -it --net redis-cluster-network --ip 172.10.0.91 -p 8001:6379 --name redis-test1 ubuntu/dev:v1.2
docker run -it -d --net redis-cluster-network --ip 172.10.0.92 -p 8002:6379 --name redis-test2 ubuntu/dev:v1.2
docker run -it -d --net redis-cluster-network --ip 172.10.0.93 -p 8003:6379 --name redis-test3 ubuntu/dev:v1.2
docker run -it -d --net redis-cluster-network --ip 172.10.0.94 -p 8004:6379 --name redis-test4 ubuntu/dev:v1.2
docker run -it -d --net redis-cluster-network --ip 172.10.0.95 -p 8005:6379 --name redis-test5 ubuntu/dev:v1.2
docker run -it -d --net redis-cluster-network --ip 172.10.0.96 -p 8006:6379 --name redis-test6 ubuntu/dev:v1.2


docker run -it -d --net redis-cluster-network --ip 172.10.0.100 --name ruby11 -i -d redis-ruby:v1

gem install redis  #  --version 3.0.7

./redis-trib.rb  create --replicas 1  172.10.0.91:6379 172.10.0.92:6379 172.10.0.93:6379 172.10.0.94:6379 172.10.0.95:6379 172.10.0.96:6379
redis-cli --cluster create 172.10.0.91:6379 172.10.0.92:6379 172.10.0.93:6379 172.10.0.94:6379 172.10.0.95:6379 172.10.0.96:6379 --cluster-replicas 1


top命令
第一行：
	load average: 0.00, 0.00, 0.00 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。
load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。

第三行：cpu状态
    0.3% us — 用户空间占用CPU的百分比。
    0.0% sy — 内核空间占用CPU的百分比。
    0.0% ni — 改变过优先级的进程占用CPU的百分比
    99.7% id — 空闲CPU百分比
    0.0% wa — IO等待占用CPU的百分比
    0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比
    0.0% si — 软中断（Software Interrupts）占用CPU的百分比

监控java线程数：
ps -eLf | grep java | wc -l

监控网络客户连接数：
netstat -n | grep tcp | grep 侦听端口 | wc -l

ls /proc/PID/task | wc -l
在linux中还有一个命令pmap，来输出进程内存的状况，可以用来分析线程堆栈：
pmap PID


1，使用命令top -p <pid> ，显示你的Java进程的内存情况，pid是你的java进程号，比如4977
2，按shift+h --> H，获取每个线程的内存情况 
2.5  1+2 可以直接使用top -Hp pid来查看

3，找到内存和cpu占用最高的线程pid，比如4977 
4，执行 System.out.println(Integer.toHexString(4977));  或者使用printf "%x\n" 4977   得到 0x1371 ,此为线程id的十六进制 
5，执行 jstack 4977|grep -A 10 1371，得到线程堆栈信息中1371这个线程所在行的后面10行 
6，查看对应的堆栈信息找出可能存在问题的代码





Kernel space 是 Linux 内核的运行空间，User space 是用户程序的运行空间。为了安全，它们是隔离的，即使用户的程序崩溃了，内核也不受影响。...
内核空间和用户空间，内核空间是内核代码运行的地方，用户空间是用户程序代码运行的地方。当进程运行在内核空间时就处于内核态，当进程运行在用户空间时就处于用户态。

str = "my string" // 用户空间
x = x + 2
file.write(str) // 切换到内核空间
y = x + 4 // 切换回用户空间




零拷贝：
read(file, tmp_buf, len);
write(socket, tmp_buf, len);


步骤一：读系统调用会导致从【用户模式】到【内核模式】的上下文切换。第一个复制由DMA引擎执行，它读取磁盘中的文件内容并将其存储到内核地址空间缓冲区中。

第二步：将数据从【内核缓冲区】复制到【用户缓冲区】，read系统调用返回。调用的返回导致了从内核返回到用户模式的上下文切换，现在，数据存储在用户地址空间缓冲区中，它可以再次开始向下移动。

第三步:write系统调用导致从用户模式到内核模式的上下文切换，执行第三个复制，将数据再次放入内核地址空间缓冲区中。但是这一次，数据被放入一个不同的缓冲区，这个缓冲区是与套接字相关联的。

第四步：写系统调用返回，创建第四个上下文切换。DMA引擎将数据从内核缓冲区传递到协议engin时，第四个复制发生了独立和异步的情况。你可能会问自己，“你说的独立和异步是什么意思？
”在调用返回之前，数据不是传输的吗？”  实际上，调用返回并不能保证传输;它甚至不能保证传输的开始。它只是意味着以太网驱动程序在其队列中有空闲的描述符并接受了我们的传输数据 ，在我们的之前可能会有很多的数据包在排队。除非驱动/硬件实现了优先级环或队列，否则数据将以先入先出的方式传输。（图1中派生的DMA copy表明了最后一个复制可以被延迟的事实）。

正如您所看到的，大量的数据复制并不是真正需要的。可以消除一些重复，以减少开销并提高性能。 作为一名驱动开发人员，我使用的硬件具有一些非常高级的特性。一些硬件可以完全绕过主存，直接将数据传输到另一个设备上。 该特性消除了系统内存中的复制，这是一件很好的事情，但并不是所有的硬件都支持它。还有一个问题是，磁盘上的数据必须重新打包以供网络使用，这带来了一些复杂的问题。 为了消除开销，我们可以从消除内核和用户缓冲区之间的一些复制开始。
